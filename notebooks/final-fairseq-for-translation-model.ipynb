{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VASJW25a-Ei5"
   },
   "source": [
    "# REFERENCES\n",
    "# 200 epochs!\n",
    "https://colab.research.google.com/drive/1HcR35DLkNszQyGv_29bYVR9EMVwERqGI?usp=sharing\n",
    "\n",
    "https://github.com/AliAbdien/Transformer-based-Machine-Translation-from-Scratch/blob/main/\n",
    "\n",
    "\n",
    "https://github.com/ZindiAfrica/Natural-Language-Processing-NLP/blob/main/Competition-Solutions/Text/AI4D%20Takwimu%20Lab%20Machine%20Translation%20Challenge/Solution%203/zindi_complete_notebook.ipynbtransformer-based-machine-translation-from-scratch.ipynb\n",
    "\n",
    "\n",
    "\n",
    "https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c64c91cf87c13c0e83586b8e66e4d74e/translation_transformer.ipynb#scrollTo=r-sOzNEz62v8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U96IO9APGH_l"
   },
   "source": [
    "# LIBRARIES  SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "copzwkTZfdYC",
    "outputId": "1cb4d97c-911e-4ed0-dfe5-e849288c96b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install --upgrade \"pip<24.1\"\\n!pip install -q transformers\\n!pip install -q huggingface_hub \\n!pip install -q datasets \\n!pip install -q wandb \\n!pip install -q ctranslate2 \\n!pip install -q sentencepiece \\n!pip install -q sacresacrebleu  \\n!pip install -q sacremoses\\n! git clone https://github.com/pytorch/fairseq # fairseq -- For training and evaluation of the model\\n%cd fairseq\\n! pip install --editable ./\\n%cd .. \\n# '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all files and directories inside /kaggle/working\n",
    "#!rm -rf*\n",
    "!find . -mindepth 1 -maxdepth 1 ! -name 'fairseq' -exec rm -rf {} + # except fairseq\n",
    "\n",
    "\"\"\"!pip install --upgrade \"pip<24.1\"\n",
    "!pip install -q transformers\n",
    "!pip install -q huggingface_hub \n",
    "!pip install -q datasets \n",
    "!pip install -q wandb \n",
    "!pip install -q ctranslate2 \n",
    "!pip install -q sentencepiece \n",
    "!pip install -q sacresacrebleu  \n",
    "!pip install -q sacremoses\n",
    "! git clone https://github.com/pytorch/fairseq # fairseq -- For training and evaluation of the model\n",
    "%cd fairseq\n",
    "! pip install --editable ./\n",
    "%cd .. \n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5Awt7fIo7fWi"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict, Translation,Dataset,Features,load_metric, concatenate_datasets\n",
    "import re\n",
    "import ctranslate2\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import uuid\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlseHTBQ-OCU"
   },
   "source": [
    "# ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0Xt7S_3D2wK",
    "outputId": "409a4def-00b1-4ad2-e7e6-e4778e42fd10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from google.colab import drive,files\\ndrive.mount('/content/drive')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from google.colab import drive,files\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "Pzt01bnIfy2s",
    "outputId": "8d2b0bd2-8484-4eab-fc83-4c0ce816568e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seed = 42   # robust\\ntorch.manual_seed(seed)\\nnp.random.seed(seed)\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nprint(DEVICE)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"seed = 42   # robust\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CakFOHNW-68O",
    "outputId": "94317c26-a386-41c3-e0dd-5c28fcacff59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#from transformers import pipeline\n",
    "#import os\n",
    "#hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login('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',\n",
    "      #add_to_git_credential=True\n",
    "      )\n",
    "\n",
    "# # Log into Hugging Face\n",
    "#!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9YRiMzHdafl",
    "outputId": "b3ea43d1-f9f3-4d3d-c5c9-39d46cfb4048"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import wandb\\nwandb_key=r'cdb9b78436f43df2e5db3dc20d9eb76c7e7dfd63'\\nwandb.login(key=wandb_key)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import wandb\n",
    "wandb_key=r'cdb9b78436f43df2e5db3dc20d9eb76c7e7dfd63'\n",
    "wandb.login(key=wandb_key)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GPQ2_S3f85n"
   },
   "source": [
    "# DEFINE CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NyLJPl0Cf85o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure set up and all paths defined:\n",
      "\n",
      "Output directory created: True\n",
      "SentencePiece directory created: True\n",
      "Trained model directory created: True\n",
      "CTranslate2 model directory created: True\n",
      "Hugging Face repository path: JuliusFx/dyula-french-nmt-8-aug\n",
      "\n",
      "Paths for datasets:\n",
      "train_dyu: preprocessing/out/fr-dyu.train.dyu\n",
      "train_fr: preprocessing/out/fr-dyu.train.fr\n",
      "dev_dyu: preprocessing/out/fr-dyu.dev.dyu\n",
      "dev_fr: preprocessing/out/fr-dyu.dev.fr\n",
      "test_dyu: preprocessing/out/fr-dyu.test.dyu\n"
     ]
    }
   ],
   "source": [
    "## Set up directories and paths\n",
    "\n",
    "# Define languages\n",
    "src_lang = \"dyu\"\n",
    "trg_lang = \"fr\"\n",
    "\n",
    "# Define regex pattern for characters to remove\n",
    "chars_to_remove_regex = r'[!\"&\\(\\),-./:;=?+.\\n\\[\\]«»]'\n",
    "\n",
    "# Define paths for datasets\n",
    "official_dataset_repo = \"data354/Koumankan_mt_dyu_fr\"\n",
    "\n",
    "# Define output directories\n",
    "output_dir = \"preprocessing/out\"\n",
    "sp_dir = \"sp\"\n",
    "trained_model_dir = \"trained_model\"\n",
    "destination_dir = \"ct2_model\"\n",
    "hf_repo = \"JuliusFx/dyula-french-nmt-8-aug\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(sp_dir, exist_ok=True)\n",
    "os.makedirs(trained_model_dir, exist_ok=True)\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define paths for exported data\n",
    "paths = {\n",
    "    'train_dyu': os.path.join(output_dir, 'fr-dyu.train.dyu'),\n",
    "    'train_fr': os.path.join(output_dir, 'fr-dyu.train.fr'),\n",
    "    'dev_dyu': os.path.join(output_dir, 'fr-dyu.dev.dyu'),\n",
    "    'dev_fr': os.path.join(output_dir, 'fr-dyu.dev.fr'),\n",
    "    'test_dyu': os.path.join(output_dir, 'fr-dyu.test.dyu'),  # Add test Dyula sentences to increase vocab\n",
    "}\n",
    "\n",
    "# Print confirmation of directories and paths\n",
    "# Verify directory creation\n",
    "print(\"Directory structure set up and all paths defined:\\n\")\n",
    "print(f\"Output directory created: {os.path.isdir(output_dir)}\")\n",
    "print(f\"SentencePiece directory created: {os.path.isdir(sp_dir)}\")\n",
    "print(f\"Trained model directory created: {os.path.isdir(trained_model_dir)}\")\n",
    "print(f\"CTranslate2 model directory created: {os.path.isdir(destination_dir)}\")\n",
    "print(f\"Hugging Face repository path: {hf_repo}\\n\")\n",
    "print(\"Paths for datasets:\")\n",
    "for key, path in paths.items():\n",
    "    print(f\"{key}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKVPDI-O-zc1"
   },
   "source": [
    "# GET DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZh-EKCkf85p"
   },
   "source": [
    "## Extra train_dfset\n",
    "This is an extra train_dfset provided by facebook. It has around 2000 samples. But since Zindi are not allowing extra train_df we do not use it in training our model.\n",
    "The train_df might also be from different topics and unclean compared to officially provided train_df. So it is a good thing not to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "fea0c640727140b59bd8d16feb2ea797",
      "da1224d9e51d4fccbaf4e531d682acca",
      "a2c0a2f341624db4943adae852fb266f",
      "ad7cb16de5b44e24bfe80aeaa06d56bb",
      "78e3559e9ad1451f90a8a08e5c43fa9d",
      "f3fa9f8f02bd4be38f90e330629b5d40",
      "7712ab57846b4b0f8492ff04d90cc5d8",
      "1df6de2e52fd4666b14604ed07ff4a34",
      "2723d4d96f85429d966c2cc727840318",
      "c72b8dedae1741b1b9d614e77c6d4c34",
      "bdd8f2e5580242d0943a855a96d9816a"
     ]
    },
    "id": "m5bb6CsBf85q",
    "outputId": "14305854-2419-43c1-e00e-75dbfb95cc82"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db30b1548682470383281b9529ecb3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'translation'],\n",
      "        num_rows: 2009\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ID': 'ID_222386669833939830868706080990304863149',\n",
       " 'translation': {'dyu': \"Tinninlo, Stanford Sanfè kalansoba Tagabolo foura kanso lônikèla-o ya lacé ko bôfèncourakan diagnôstiki minin coura labô bi sé ka fankisè-o bô sougouya sougouya: gouli kélé mine bi sé ka dilan ni lankiri sèbèninôbônan-o kangwè ta i' na fô U. S kèmèna a kélé kélé.\",\n",
       "  'fr': \"Des scientifiques de l’école de médecine de l’université de Stanford ont annoncé ce lundi la création d'un nouvel outil de diagnostic, qui permettrait de différencier les cellules en fonction de leur type. Il s'agit d'une petit puce imprimable, qui peut être produite au moyen d'une imprimante à jet d'encre standard, pour un coût d'environ un cent de dollar pièce.\"}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to transform entries in the dataset\n",
    "def transform_entry(entry):\n",
    "    new_id = f\"ID_{uuid.uuid4().int}\"\n",
    "    translation = {\n",
    "        'dyu': entry['dyula'],\n",
    "        'fr': entry['french']\n",
    "    }\n",
    "    return {'ID': new_id, 'translation': translation}\n",
    "\n",
    "# Function to transform datasets and remove unnecessary columns\n",
    "def transform_dataset(dataset):\n",
    "    columns_to_remove = ['dyula', 'french']\n",
    "    return dataset.map(transform_entry, remove_columns=columns_to_remove)\n",
    "\n",
    "# Load the dataset (adjust the name if needed)\n",
    "dataset = load_dataset(\"facebook/flores\", \"all\", trust_remote_code=True)\n",
    "\n",
    "# Extract Dyula and French sentences from each dataset directly\n",
    "def extract_dyula_french(example):\n",
    "    return {\n",
    "        'dyula': example['sentence_dyu_Latn'],\n",
    "        'french': example['sentence_fra_Latn']\n",
    "    }\n",
    "\n",
    "# Process and extract sentences from all datasets\n",
    "dyula_french_datasets = {\n",
    "    split: dataset[split].map(extract_dyula_french) for split in ['dev', 'devtest']\n",
    "}\n",
    "\n",
    "# Combine all datasets into one dataset\n",
    "combined_data = {\n",
    "    'dyula': [],\n",
    "    'french': []\n",
    "}\n",
    "for split, data in dyula_french_datasets.items():\n",
    "    combined_data['dyula'].extend(data['dyula'])\n",
    "    combined_data['french'].extend(data['french'])\n",
    "\n",
    "# Create a combined Dataset\n",
    "combined_dataset = Dataset.from_dict(combined_data)\n",
    "\n",
    "# Transform the combined dataset\n",
    "transformed_combined_dataset = transform_dataset(combined_dataset)\n",
    "\n",
    "# Create the final DatasetDict with the transformed combined dataset as the train dataset\n",
    "extra_dataset = DatasetDict({\n",
    "    'train': transformed_combined_dataset\n",
    "})\n",
    "\n",
    "# Print the final dataset\n",
    "print(extra_dataset)\n",
    "display(extra_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqgaM7U8f85q"
   },
   "source": [
    "## Official train_dfset\n",
    "This is a cleaner official train_dfset provided by zindi through Koumanka. We use this for training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbagjxYmZ9rC",
    "outputId": "d34f8618-1c87-44c3-a070-b8c80fc4e052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 8065\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 1471\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 1393\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "official_dataset = load_dataset(official_dataset_repo)\n",
    "official_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "RiGUcztb_uJl",
    "outputId": "40aec874-a84f-4ace-a571-4bcbb2afa19c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'official_dataset[\\'train\\'].to_pandas().to_csv(\\'train.csv\\', index=False)\\nofficial_dataset[\\'validation\\'].to_pandas().to_csv(\\'validation.csv\\', index=False)\\nofficial_dataset[\\'test\\'].to_pandas().to_csv(\\'test.csv\\', index=False)\\nprint(f\"Data exported to {train_csv_path}, {validation_csv_path}, and {test_csv_path}\")#'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each dataset to a pandas DataFrame and then export to CSV\n",
    "\"\"\"official_dataset['train'].to_pandas().to_csv('train.csv', index=False)\n",
    "official_dataset['validation'].to_pandas().to_csv('validation.csv', index=False)\n",
    "official_dataset['test'].to_pandas().to_csv('test.csv', index=False)\n",
    "print(f\"Data exported to {train_csv_path}, {validation_csv_path}, and {test_csv_path}\")#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvBhALcuf85s"
   },
   "source": [
    "## Combine train_dfsets..??\n",
    "We can optioinally combine the train_dfset by changing the flag combine to True. Because extra train_df is prohibitted we flag false and just use the official train_dfset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHeWfQwIf85s",
    "outputId": "66862daf-22da-4bd3-9508-b73b6cc81cb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 8065\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 1471\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 1393\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine=False\n",
    "if combine:\n",
    "    dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'ID': official_dataset['train']['ID'] + extra_dataset['train']['ID'],\n",
    "        'translation': official_dataset['train']['translation'] + extra_dataset['train']['translation']}),\n",
    "\n",
    "    'validation': Dataset.from_dict({\n",
    "        'ID': official_dataset['validation']['ID'] ,\n",
    "        'translation': official_dataset['validation']['translation']}),\n",
    "\n",
    "    'test': Dataset.from_dict({\n",
    "        'ID': official_dataset['test']['ID'],\n",
    "        'translation': official_dataset['test']['translation']})\n",
    "    })\n",
    "else:\n",
    "    dataset=official_dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSF78z5yf85t"
   },
   "source": [
    "## Use Some of Validation Sentences..??\n",
    "Since train train_dfset is small we use some validation train_dfset for training around 1400 samples. THe downside is we wont be able to thoroughly test how our model performs. The advantage is that we might have a longer train train_dfset around 9500 from 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SltI13AA_3_p",
    "outputId": "888efbc8-6f52-4df0-d8d4-6574ca0fb4be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 9526\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 1393\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_some_valiation=True\n",
    "if use_some_valiation:\n",
    "    # Number of validation samples to move in percentage\n",
    "    percentage_samples_to_move = .80\n",
    "\n",
    "    # Determine the number of samples to move\n",
    "    num_samples_to_move = int(percentage_samples_to_move * len(dataset['validation']))\n",
    "    num_samples_to_move=1461\n",
    "\n",
    "    # Select the samples to move\n",
    "    samples_to_add_to_train = dataset['validation'].select(range(num_samples_to_move))\n",
    "\n",
    "    # Update the validation data to exclude the moved samples\n",
    "    updated_validation_data = dataset['validation'].select(range(num_samples_to_move, len(dataset['validation'])))\n",
    "\n",
    "    # Update the train data to include the moved samples from validation\n",
    "    updated_train_data = concatenate_datasets([dataset['train'], samples_to_add_to_train])\n",
    "\n",
    "    # Create the new DatasetDict with the updated datasets\n",
    "    dataset = DatasetDict({\n",
    "        'train': updated_train_data,\n",
    "        'validation': updated_validation_data,\n",
    "        'test': official_dataset['test']\n",
    "    })\n",
    "\n",
    "\n",
    "else:\n",
    "    dataset=dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RiFdefzF_jcm",
    "outputId": "1c1e1a72-53a1-4ea7-daac-b1e4bdad6c46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 9526\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'translation'],\n",
       "        num_rows: 1393\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_special_characters(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters based on regex\n",
    "    text = re.sub(chars_to_remove_regex, ' ', text)\n",
    "\n",
    "    # Replace multiple consecutive dots with a single dot\n",
    "    text = re.sub(r'\\.{2,}', '', text)\n",
    "\n",
    "    # Replace typographic apostrophe with straight apostrophe\n",
    "    text = text.replace(\"’\", \"'\")\n",
    "\n",
    "    # Remove em dashes or other dashes if needed\n",
    "    text = text.replace('—', '')\n",
    "\n",
    "    # Replace ellipses with a single dot or handle as needed\n",
    "    text = text.replace('…', '')\n",
    "\n",
    "    # Remove extra white spaces (convert multiple spaces to a single space)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text(batch):\n",
    "    # process source text\n",
    "    batch['translation'][src_lang] = remove_special_characters(batch['translation'][src_lang])\n",
    "    # process target text\n",
    "    batch['translation'][trg_lang] = remove_special_characters(batch['translation'][trg_lang])\n",
    "    return batch\n",
    "\n",
    "# Assuming `data` is a DatasetDict or similar structure\n",
    "dataset = dataset.map(clean_text)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "J0RL3lVTAK5w",
    "outputId": "b04bd507-9c85-4af5-9919-3aa31b4c157d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame (length: 9433):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dyu</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_18897661270129</td>\n",
       "      <td>a bi ji min na</td>\n",
       "      <td>il boit de l'eau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_18479132727846</td>\n",
       "      <td>a le dalakolontɛ lon bɛ</td>\n",
       "      <td>il se plaint toujours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID                      dyu                     fr\n",
       "0  ID_18897661270129           a bi ji min na       il boit de l'eau\n",
       "1  ID_18479132727846  a le dalakolontɛ lon bɛ  il se plaint toujours"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataFrame (length: 10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dyu</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_19561563644823</td>\n",
       "      <td>nou nou ti na na tougou</td>\n",
       "      <td>la mer de behring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_17379610645803</td>\n",
       "      <td>madam kalvɛz ma</td>\n",
       "      <td>à madame calvez</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID                      dyu                 fr\n",
       "0  ID_19561563644823  nou nou ti na na tougou  la mer de behring\n",
       "1  ID_17379610645803          madam kalvɛz ma    à madame calvez"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test DataFrame (length: 1393):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dyu</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_17345911362699</td>\n",
       "      <td>an kelen duron le tun be yi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_173626847.3381</td>\n",
       "      <td>o ka papiye farana</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID                          dyu fr\n",
       "0  ID_17345911362699  an kelen duron le tun be yi  0\n",
       "1  ID_173626847.3381           o ka papiye farana  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert each dataset to a pandas DataFrame\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "validation_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Function to process 'translation' column\n",
    "def process_translation(df):\n",
    "    translations = pd.json_normalize(df['translation'])\n",
    "    df = df.drop(columns=['translation'])\n",
    "    df = pd.concat([df, translations], axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "train_df = process_translation(train_df)\n",
    "train_df = train_df.drop_duplicates(subset=['dyu', 'fr'])\n",
    "train_df = train_df.drop_duplicates(subset=['dyu'])\n",
    "\n",
    "validation_df = process_translation(validation_df)\n",
    "validation_df = validation_df.drop_duplicates(subset=['dyu', 'fr'])\n",
    "validation_df = validation_df.drop_duplicates(subset=['dyu'])\n",
    "\n",
    "test_df = process_translation(test_df)\n",
    "#test_df['dyu'].to_csv('test.csv', index=False,encoding=\"utf-8\")\n",
    "\n",
    "# Print the DataFrames to verify\n",
    "print(f\"Train DataFrame (length: {len(train_df)}):\")\n",
    "display(train_df.head(2))\n",
    "\n",
    "print(f\"\\nValidation DataFrame (length: {len(validation_df)}):\")\n",
    "display(validation_df.head(2))\n",
    "\n",
    "print(f\"\\nTest DataFrame (length: {len(test_df)}):\")\n",
    "display(test_df.head(2))\n",
    "\n",
    "# Combine the 'dyu' columns from train_df, test_df, and validation_df into a single DataFrame\n",
    "dyu_text = pd.concat([train_df['dyu'], test_df['dyu'], validation_df['dyu']], ignore_index=True)\n",
    "\n",
    "# Combine the 'fr' columns from train_df and validation_df into a single DataFrame\n",
    "fr_text = pd.concat([train_df['fr'], validation_df['fr']], ignore_index=True)\n",
    "\n",
    "# Combine the 'fr' & 'dyu' columns from train_df and validation_df into a single DataFrame\n",
    "smpl = pd.concat([validation_df, train_df])\n",
    "#smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import csv\\n\\n# Define the list\\nsentences = test_df.dyu.tolist()\\n\\n# Write the list to a CSV file\\nwith open('sentences.csv', 'w', newline='', encoding='utf-8') as file:\\n    writer = csv.writer(file, quoting=csv.QUOTE_ALL)\\n    writer.writerow(sentences)\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import csv\n",
    "\n",
    "# Define the list\n",
    "sentences = test_df.dyu.tolist()\n",
    "\n",
    "# Write the list to a CSV file\n",
    "with open('sentences.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(sentences)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can Augmentation Help Improve Matters..??\n",
    "Since train dataset is small we make an effort to increase it using augmentation techniques. The advantage is that we might have a longer train dataset around 9500 from 2000.We might a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cf50daf34c44ec893c892ab0ff178d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c583e9e9b50a48f4be60084d54d7a3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c27650d6076474eab813f0e40e8c769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8207289938004318aa8bdbfc8ed7733b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea68856c05e40a8be886268fcb7fa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model=\"camembert-base\", device=-1) # device=0\n",
    "\n",
    "# replacing words in sentence by new words generated by camembert\n",
    "def augment_sentence(sent, n):\n",
    "    french = sent.split(\" \")\n",
    "    replace_idxs = random.sample(range(len(french)), n)\n",
    "    for replace_idx in replace_idxs:\n",
    "        original = french[replace_idx]\n",
    "        french[replace_idx] = nlp.tokenizer.mask_token\n",
    "        out = nlp(\" \".join(french))\n",
    "        draw = random.choices(out, cum_weights=list(map(lambda x: x[\"score\"], out)), k=1)\n",
    "        french[replace_idx] = draw[0][\"token_str\"]\n",
    "    return \" \".join(french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing Batches: 100%|██████████| 10/10 [13:25<00:00, 80.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dyu</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cc883282-cbf3-4b74-89c4-64216f6a8af9</td>\n",
       "      <td>a bi ji min na</td>\n",
       "      <td>il boit de l'eau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e42b39f2-330f-447d-a3c5-23cf92dba80d</td>\n",
       "      <td>a le dalakolontɛ lon bɛ</td>\n",
       "      <td>il se plaint toujours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72ed4896-ddb3-41a0-b504-bd31fff17e3c</td>\n",
       "      <td>mun fɛn dɔ</td>\n",
       "      <td>Faire quelque chose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>944caa71-fca0-4041-81aa-39bccb970637</td>\n",
       "      <td>o bɛ bi bɔra fo gubeta</td>\n",
       "      <td>tous sortent leur gubetta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3cf4588d-a1d0-414e-94a7-1755acd1206b</td>\n",
       "      <td>a ale lo bi da bugɔ la</td>\n",
       "      <td>ah oui lui il sonne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9428</th>\n",
       "      <td>0997b12f-f47f-49a5-8a99-e3ab3554fc1c</td>\n",
       "      <td>ne sera kalan nan</td>\n",
       "      <td>Je suis en train de lire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9429</th>\n",
       "      <td>94265f04-2f34-43db-a102-d10fb2065a84</td>\n",
       "      <td>belébele wa fiman</td>\n",
       "      <td>Le grand ou le petit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9430</th>\n",
       "      <td>8c412ee5-90bf-4cac-9440-0042fefdcdf7</td>\n",
       "      <td>bara kaɲi ka kɛra</td>\n",
       "      <td>il faut bien travailler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9431</th>\n",
       "      <td>2ac04e88-7c5a-4a04-9140-09d4aecc3261</td>\n",
       "      <td>an wa denbaya ye denbayadeni ye</td>\n",
       "      <td>Une famille est une petite famille</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9432</th>\n",
       "      <td>871137c7-b8b9-4aa5-8321-5347caad8b97</td>\n",
       "      <td>saya tugu la a kɔ fo asansɛr bonda la</td>\n",
       "      <td>sara est conduite jusqu'à une porte d'ascenseur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9433 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ID  \\\n",
       "0     cc883282-cbf3-4b74-89c4-64216f6a8af9   \n",
       "1     e42b39f2-330f-447d-a3c5-23cf92dba80d   \n",
       "2     72ed4896-ddb3-41a0-b504-bd31fff17e3c   \n",
       "3     944caa71-fca0-4041-81aa-39bccb970637   \n",
       "4     3cf4588d-a1d0-414e-94a7-1755acd1206b   \n",
       "...                                    ...   \n",
       "9428  0997b12f-f47f-49a5-8a99-e3ab3554fc1c   \n",
       "9429  94265f04-2f34-43db-a102-d10fb2065a84   \n",
       "9430  8c412ee5-90bf-4cac-9440-0042fefdcdf7   \n",
       "9431  2ac04e88-7c5a-4a04-9140-09d4aecc3261   \n",
       "9432  871137c7-b8b9-4aa5-8321-5347caad8b97   \n",
       "\n",
       "                                        dyu  \\\n",
       "0                            a bi ji min na   \n",
       "1                   a le dalakolontɛ lon bɛ   \n",
       "2                                mun fɛn dɔ   \n",
       "3                    o bɛ bi bɔra fo gubeta   \n",
       "4                    a ale lo bi da bugɔ la   \n",
       "...                                     ...   \n",
       "9428                      ne sera kalan nan   \n",
       "9429                      belébele wa fiman   \n",
       "9430                      bara kaɲi ka kɛra   \n",
       "9431        an wa denbaya ye denbayadeni ye   \n",
       "9432  saya tugu la a kɔ fo asansɛr bonda la   \n",
       "\n",
       "                                                   fr  \n",
       "0                                    il boit de l'eau  \n",
       "1                               il se plaint toujours  \n",
       "2                                 Faire quelque chose  \n",
       "3                           tous sortent leur gubetta  \n",
       "4                                 ah oui lui il sonne  \n",
       "...                                               ...  \n",
       "9428                         Je suis en train de lire  \n",
       "9429                             Le grand ou le petit  \n",
       "9430                          il faut bien travailler  \n",
       "9431               Une famille est une petite famille  \n",
       "9432  sara est conduite jusqu'à une porte d'ascenseur  \n",
       "\n",
       "[9433 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "\n",
    "# Initialize the NLP pipeline for GPU usage\n",
    "nlp = pipeline(\"fill-mask\", model=\"camembert-base\", device=0)  # Use device=-1 for CPU\n",
    "\n",
    "def augment_sentence(sent, n):\n",
    "    french = sent.split(\" \")\n",
    "    replace_idxs = random.sample(range(len(french)), n)\n",
    "    for replace_idx in replace_idxs:\n",
    "        original = french[replace_idx]\n",
    "        french[replace_idx] = nlp.tokenizer.mask_token\n",
    "        out = nlp(\" \".join(french))\n",
    "        draw = random.choices(out, cum_weights=list(map(lambda x: x[\"score\"], out)), k=1)\n",
    "        french[replace_idx] = draw[0][\"token_str\"]\n",
    "    return \" \".join(french)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1000  # Number of rows per batch\n",
    "augment_data = True\n",
    "\n",
    "# Convert your DataFrame to a Dataset\n",
    "data = Dataset.from_pandas(data)\n",
    "\n",
    "# Create an empty list to store augmented data\n",
    "augmented_sents = []\n",
    "\n",
    "if augment_data:\n",
    "    # Number of rows in the Dataset\n",
    "    total_rows = len(data)\n",
    "    \n",
    "    # Process data in batches\n",
    "    for start in tqdm(range(0, total_rows, batch_size), desc=\"Processing Batches\"):\n",
    "        end = min(start + batch_size, total_rows)\n",
    "        batch = data.select(range(start, end))  # Select the batch\n",
    "        \n",
    "        # Process each row in the batch\n",
    "        for row in batch:\n",
    "            sent = row['fr']\n",
    "            augmented_sent = augment_sentence(sent, n=max(1, len(sent.split(\" \")) // 3))\n",
    "            # Generate a UUID for each augmented sentence\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            augmented_sents.append((unique_id, row['dyu'], augmented_sent))\n",
    "\n",
    "# Convert augmented_sents into a DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_sents, columns=['ID', 'dyu', 'fr'])\n",
    "\n",
    "# Display the new DataFrame\n",
    "augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dyu</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_18897661270129</td>\n",
       "      <td>a bi ji min na</td>\n",
       "      <td>il boit de l'eau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_18479132727846</td>\n",
       "      <td>a le dalakolontɛ lon bɛ</td>\n",
       "      <td>il se plaint toujours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_18164131280307</td>\n",
       "      <td>mun fɛn dɔ</td>\n",
       "      <td>quoi quelque chose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_18344573728152</td>\n",
       "      <td>o bɛ bi bɔra fo gubeta</td>\n",
       "      <td>tous sortent excepté gubetta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_18127342282717</td>\n",
       "      <td>a ale lo bi da bugɔ la</td>\n",
       "      <td>ah c'est lui il sonne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9521</th>\n",
       "      <td>ID_17352313636693</td>\n",
       "      <td>ne sera kalan nan</td>\n",
       "      <td>je suis en train de lire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9522</th>\n",
       "      <td>ID_18824297638014</td>\n",
       "      <td>belébele wa fiman</td>\n",
       "      <td>le grand ou le petit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9523</th>\n",
       "      <td>ID_18342329640632</td>\n",
       "      <td>bara kaɲi ka kɛra</td>\n",
       "      <td>il faut y travailler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9524</th>\n",
       "      <td>ID_18064988641972</td>\n",
       "      <td>an wa denbaya ye denbayadeni ye</td>\n",
       "      <td>notre famille est une petite famille</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9525</th>\n",
       "      <td>ID_18428701643712</td>\n",
       "      <td>saya tugu la a kɔ fo asansɛr bonda la</td>\n",
       "      <td>sara le suivi jusqu'à une porte d'ascenseur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9433 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID                                    dyu  \\\n",
       "0     ID_18897661270129                         a bi ji min na   \n",
       "1     ID_18479132727846                a le dalakolontɛ lon bɛ   \n",
       "2     ID_18164131280307                             mun fɛn dɔ   \n",
       "3     ID_18344573728152                 o bɛ bi bɔra fo gubeta   \n",
       "4     ID_18127342282717                 a ale lo bi da bugɔ la   \n",
       "...                 ...                                    ...   \n",
       "9521  ID_17352313636693                      ne sera kalan nan   \n",
       "9522  ID_18824297638014                      belébele wa fiman   \n",
       "9523  ID_18342329640632                      bara kaɲi ka kɛra   \n",
       "9524  ID_18064988641972        an wa denbaya ye denbayadeni ye   \n",
       "9525  ID_18428701643712  saya tugu la a kɔ fo asansɛr bonda la   \n",
       "\n",
       "                                               fr  \n",
       "0                                il boit de l'eau  \n",
       "1                           il se plaint toujours  \n",
       "2                              quoi quelque chose  \n",
       "3                    tous sortent excepté gubetta  \n",
       "4                           ah c'est lui il sonne  \n",
       "...                                           ...  \n",
       "9521                     je suis en train de lire  \n",
       "9522                         le grand ou le petit  \n",
       "9523                         il faut y travailler  \n",
       "9524         notre famille est une petite famille  \n",
       "9525  sara le suivi jusqu'à une porte d'ascenseur  \n",
       "\n",
       "[9433 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df\n",
    "#augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dyu</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_18897661270129</td>\n",
       "      <td>a bi ji min na</td>\n",
       "      <td>il boit de l'eau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_18479132727846</td>\n",
       "      <td>a le dalakolontɛ lon bɛ</td>\n",
       "      <td>il se plaint toujours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_18164131280307</td>\n",
       "      <td>mun fɛn dɔ</td>\n",
       "      <td>quoi quelque chose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_18344573728152</td>\n",
       "      <td>o bɛ bi bɔra fo gubeta</td>\n",
       "      <td>tous sortent excepté gubetta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_18127342282717</td>\n",
       "      <td>a ale lo bi da bugɔ la</td>\n",
       "      <td>ah c'est lui il sonne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9428</th>\n",
       "      <td>0997b12f-f47f-49a5-8a99-e3ab3554fc1c</td>\n",
       "      <td>ne sera kalan nan</td>\n",
       "      <td>Je suis en train de lire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9429</th>\n",
       "      <td>94265f04-2f34-43db-a102-d10fb2065a84</td>\n",
       "      <td>belébele wa fiman</td>\n",
       "      <td>Le grand ou le petit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9430</th>\n",
       "      <td>8c412ee5-90bf-4cac-9440-0042fefdcdf7</td>\n",
       "      <td>bara kaɲi ka kɛra</td>\n",
       "      <td>il faut bien travailler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9431</th>\n",
       "      <td>2ac04e88-7c5a-4a04-9140-09d4aecc3261</td>\n",
       "      <td>an wa denbaya ye denbayadeni ye</td>\n",
       "      <td>Une famille est une petite famille</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9432</th>\n",
       "      <td>871137c7-b8b9-4aa5-8321-5347caad8b97</td>\n",
       "      <td>saya tugu la a kɔ fo asansɛr bonda la</td>\n",
       "      <td>sara est conduite jusqu'à une porte d'ascenseur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18866 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ID  \\\n",
       "0                        ID_18897661270129   \n",
       "1                        ID_18479132727846   \n",
       "2                        ID_18164131280307   \n",
       "3                        ID_18344573728152   \n",
       "4                        ID_18127342282717   \n",
       "...                                    ...   \n",
       "9428  0997b12f-f47f-49a5-8a99-e3ab3554fc1c   \n",
       "9429  94265f04-2f34-43db-a102-d10fb2065a84   \n",
       "9430  8c412ee5-90bf-4cac-9440-0042fefdcdf7   \n",
       "9431  2ac04e88-7c5a-4a04-9140-09d4aecc3261   \n",
       "9432  871137c7-b8b9-4aa5-8321-5347caad8b97   \n",
       "\n",
       "                                        dyu  \\\n",
       "0                            a bi ji min na   \n",
       "1                   a le dalakolontɛ lon bɛ   \n",
       "2                                mun fɛn dɔ   \n",
       "3                    o bɛ bi bɔra fo gubeta   \n",
       "4                    a ale lo bi da bugɔ la   \n",
       "...                                     ...   \n",
       "9428                      ne sera kalan nan   \n",
       "9429                      belébele wa fiman   \n",
       "9430                      bara kaɲi ka kɛra   \n",
       "9431        an wa denbaya ye denbayadeni ye   \n",
       "9432  saya tugu la a kɔ fo asansɛr bonda la   \n",
       "\n",
       "                                                   fr  \n",
       "0                                    il boit de l'eau  \n",
       "1                               il se plaint toujours  \n",
       "2                                  quoi quelque chose  \n",
       "3                        tous sortent excepté gubetta  \n",
       "4                               ah c'est lui il sonne  \n",
       "...                                               ...  \n",
       "9428                         Je suis en train de lire  \n",
       "9429                             Le grand ou le petit  \n",
       "9430                          il faut bien travailler  \n",
       "9431               Une famille est une petite famille  \n",
       "9432  sara est conduite jusqu'à une porte d'ascenseur  \n",
       "\n",
       "[18866 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if augment_data:\n",
    "    train_df=pd.concat([train_df,augmented_df])\n",
    "    #train_dff=pd.concat([train_df,augmented_df])\n",
    "else:\n",
    "    pass\n",
    "#train_dff\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30Y-GEb7f85v"
   },
   "source": [
    "#  PREPARE DATA FOR FAIRSEQ TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgvhZqxmf85w",
    "outputId": "68acaef5-3b0b-4e3e-d623-e57f95a453c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation train_df exported.\n"
     ]
    }
   ],
   "source": [
    "# Export Dyula and French texts for training and validation\n",
    "train_df[['dyu']].to_csv(paths['train_dyu'], index=False, header=False)\n",
    "train_df[['fr']].to_csv(paths['train_fr'], index=False, header=False)\n",
    "validation_df[['dyu']].to_csv(paths['dev_dyu'], index=False, header=False)\n",
    "validation_df[['fr']].to_csv(paths['dev_fr'], index=False, header=False)\n",
    "test_df[['dyu']].to_csv(paths['test_dyu'], index=False, header=False) #add test dyula sentenc to increase vocab\n",
    "\n",
    "print(\"Training and validation train_df exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "w5OCadvXf85w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#Combine train_df\n",
    "!cat preprocessing/out/fr-dyu.train.dyu > sp/combined_train_df\n",
    "!cat preprocessing/out/fr-dyu.train.fr >> sp/combined_train_df\n",
    "!cat preprocessing/out/fr-dyu.dev.dyu >> sp/combined_train_df\n",
    "!cat preprocessing/out/fr-dyu.dev.fr >> sp/combined_train_df\n",
    "!cat preprocessing/out/fr-dyu.dev.fr >> sp/combined_train_df\n",
    "#!cat preprocessing/out/fr-dyu.test.dyu >> sp/combined_train_df #add test dyula sentenc to increase vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQ5PmjdCf85w",
    "outputId": "e626aed4-c993-4a34-dbc7-bf77bdf4e865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bi ji min na\n",
      "a le dalakolontɛ lon bɛ\n",
      "mun fɛn dɔ\n",
      "o bɛ bi bɔra fo gubeta\n",
      "a ale lo bi da bugɔ la\n",
      "e nafa t'a ra\n",
      "sanu baa de bi n bolo\n",
      "onhon mɛ laɲini jumanan\n",
      "kamele baba\n",
      "duguden dɔ minɛ na\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 sp/combined_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_vXkA7Mf85x",
    "outputId": "9b93f036-245c-4467-e8a1-6b5c8add4f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'sp/combined_model_2000.model': No such file or directory\n",
      "rm: cannot remove 'sp/combined_model_2000.vocab': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm sp/combined_model_2000.model sp/combined_model_2000.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ik_HU6of85x"
   },
   "source": [
    "# TRAIN SENTENCEPIECE MODEL\n",
    "-outputs the model and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "nmwXidFIf85x",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sp/combined_train_df\n",
      "  input_format: \n",
      "  model_prefix: sp/combined_model_2000\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: sp/combined_train_df\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 37762 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1102283\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.1411% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=33\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.991411\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 37762 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 37762\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 24369\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16376 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4310 size=20 all=1829 active=1795 piece=qu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2489 size=40 all=3019 active=2985 piece=el\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1728 size=60 all=4265 active=4231 piece=fa\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1322 size=80 all=5236 active=5202 piece=ment\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1102 size=100 all=6289 active=6255 piece=ɛr\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1083 min_freq=72\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=821 size=120 all=7407 active=2046 piece=mon\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=690 size=140 all=8454 active=3093 piece=to\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=611 size=160 all=9408 active=4047 piece=fo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=505 size=180 all=9965 active=4604 piece=int\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=443 size=200 all=10539 active=5178 piece=mb\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=443 min_freq=63\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=397 size=220 all=11310 active=1709 piece=ée\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=353 size=240 all=11900 active=2299 piece=ban\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=325 size=260 all=12405 active=2804 piece=mis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=301 size=280 all=12892 active=3291 piece=sou\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=286 size=300 all=13518 active=3917 piece=cet\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=286 min_freq=54\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=266 size=320 all=13999 active=1477 piece=trois\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=235 size=340 all=14424 active=1902 piece=elles\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=218 size=360 all=14822 active=2300 piece=mille\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=208 size=380 all=15265 active=2743 piece=ɲana\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=195 size=400 all=15675 active=3153 piece=uf\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=195 min_freq=47\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=180 size=420 all=15995 active=1314 piece=wagati\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=171 size=440 all=16348 active=1667 piece=cinq\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=159 size=460 all=16529 active=1848 piece=vais\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=149 size=480 all=16798 active=2117 piece=ger\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=142 size=500 all=17106 active=2425 piece=juma\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=142 min_freq=42\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=520 all=17332 active=1221 piece=gon\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=540 all=17650 active=1539 piece=jours\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=127 size=560 all=17834 active=1723 piece=quand\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=123 size=580 all=18112 active=2001 piece=vie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=118 size=600 all=18319 active=2208 piece=cor\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=118 min_freq=37\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=113 size=620 all=18545 active=1197 piece=premi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=640 all=18728 active=1380 piece=toum\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=102 size=660 all=18988 active=1640 piece=ala\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=100 size=680 all=19258 active=1910 piece=ris\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=96 size=700 all=19544 active=2196 piece=jet\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=96 min_freq=34\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=92 size=720 all=19760 active=1201 piece=ion\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=88 size=740 all=19960 active=1401 piece=deen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=84 size=760 all=20162 active=1603 piece=ami\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=780 all=20443 active=1884 piece=tcha\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=800 all=20578 active=2019 piece=bles\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=80 min_freq=30\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=820 all=20731 active=1167 piece=positi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=840 all=20939 active=1375 piece=fils\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=72 size=860 all=21100 active=1536 piece=avons\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69 size=880 all=21253 active=1689 piece=far\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=900 all=21341 active=1777 piece=veau\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=68 min_freq=27\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=920 all=21469 active=1191 piece=pes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=940 all=21623 active=1345 piece=kɔsɔbɛ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=960 all=21813 active=1535 piece=blan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=980 all=21965 active=1687 piece=notre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=1000 all=22115 active=1837 piece=br\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=59 min_freq=24\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=1020 all=22293 active=1266 piece=aller\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=1040 all=22467 active=1440 piece=wou\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=1060 all=22595 active=1568 piece=tel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=1080 all=22750 active=1723 piece=gwɛlɛ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=1100 all=22865 active=1838 piece=bois\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=52 min_freq=22\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=1120 all=22932 active=1206 piece=commune\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=1140 all=23053 active=1327 piece=hɔn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=1160 all=23109 active=1383 piece=lie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=1180 all=23277 active=1551 piece=sigɛ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=1200 all=23398 active=1672 piece=partie\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=46 min_freq=21\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=1220 all=23504 active=1276 piece=daminɛ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=1240 all=23580 active=1352 piece=principa\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=1260 all=23691 active=1463 piece=ɲɛna\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=1280 all=23818 active=1590 piece=problè\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=1300 all=23952 active=1724 piece=soir\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=42 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=1320 all=24029 active=1273 piece=tir\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=1340 all=24116 active=1360 piece=ɛri\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=1360 all=24213 active=1457 piece=kumakan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=1380 all=24364 active=1608 piece=ance\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=1400 all=24443 active=1687 piece=ɲanama\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=38 min_freq=18\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=1420 all=24501 active=1279 piece=quest\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=1440 all=24600 active=1378 piece=reti\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=1460 all=24655 active=1433 piece=pendant\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=1480 all=24764 active=1542 piece=mange\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1500 all=24852 active=1630 piece=duru\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=34 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1520 all=24926 active=1313 piece=arrivé\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=1540 all=25006 active=1393 piece=onze\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1560 all=25090 active=1477 piece=dri\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1580 all=25228 active=1615 piece=plan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1600 all=25256 active=1643 piece=question\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=31 min_freq=16\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1620 all=25339 active=1345 piece=commun\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1640 all=25456 active=1462 piece=tée\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1660 all=25512 active=1518 piece=koura\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1680 all=25519 active=1525 piece=ay\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1700 all=25594 active=1600 piece=église\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=29 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1720 all=25720 active=1406 piece=tex\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1740 all=25785 active=1471 piece=vite\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1760 all=25826 active=1512 piece=djouma\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1780 all=25865 active=1551 piece=mero\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1800 all=25907 active=1593 piece=dal\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1820 all=26022 active=1400 piece=dore\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1840 all=26077 active=1455 piece=andre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1860 all=26102 active=1480 piece=quinze\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1880 all=26172 active=1550 piece=diri\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1900 all=26247 active=1625 piece=parents\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=25 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1920 all=26314 active=1380 piece=sso\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1940 all=26385 active=1451 piece=toka\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1960 all=26436 active=1502 piece=onhon\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: sp/combined_model_2000.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: sp/combined_model_2000.vocab\n"
     ]
    }
   ],
   "source": [
    "#Train the SentencePiece model:\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='sp/combined_train_df',\n",
    "    model_prefix='sp/combined_model_2000',\n",
    "    vocab_size=2000,\n",
    "    character_coverage=0.99,\n",
    "    model_type='bpe',\n",
    "    user_defined_symbols=['▁'], # Custom symbols for special tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FOO5SLHf85y",
    "outputId": "56f76a3d-71bc-4f21-d167-143ea0cc4da1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\t0\n",
      "<s>\t0\n",
      "</s>\t0\n",
      "▁\t0\n",
      "an\t-0\n",
      "le\t-1\n",
      "en\t-2\n",
      "ou\t-3\n",
      "on\t-4\n",
      "in\t-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 sp/combined_model_2000.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qiv81o9kf85y"
   },
   "source": [
    "# SEGMENT train_dfSETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "gIQ2zrMpf85y"
   },
   "outputs": [],
   "source": [
    "#Segment the train and validation train_dfsets using the trained SentencePiece model:\n",
    "# Load SentencePiece model\n",
    "sp = spm.SentencePieceProcessor(model_file=\"sp/combined_model_2000.model\")\n",
    "\n",
    "def segment_file(in_file, out_file, model):\n",
    "    with open(in_file, \"r\", encoding=\"utf-8\") as inf, open(out_file, \"w\", encoding=\"utf-8\") as outf:\n",
    "        for line in inf:\n",
    "            outf.write(' '.join(model.encode(line.rstrip(), out_type=str)) + \"\\n\")\n",
    "\n",
    "def segment_bitext(in_prefix, out_prefix, src_lang, tgt_lang, src_model, tgt_model):\n",
    "    segment_file(f\"{in_prefix}.{src_lang}\", f\"{out_prefix}.{src_lang}\", src_model)\n",
    "    segment_file(f\"{in_prefix}.{tgt_lang}\", f\"{out_prefix}.{tgt_lang}\", tgt_model)\n",
    "\n",
    "def segment_train_dfset(in_prefix, out_prefix, src_lang, tgt_lang, src_model, tgt_model):\n",
    "    segment_bitext(f\"{in_prefix}.train\", f\"{out_prefix}/train\", src_lang, tgt_lang, src_model, tgt_model)\n",
    "    segment_bitext(f\"{in_prefix}.dev\", f\"{out_prefix}/dev\", src_lang, tgt_lang, src_model, tgt_model)\n",
    "\n",
    "# Create output directory\n",
    "target_dir = \"pretraining/sp\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Segment train and dev train_dfsets\n",
    "segment_train_dfset(\"preprocessing/out/fr-dyu\", target_dir, \"dyu\", \"fr\", sp, sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGoFc51Df85z"
   },
   "source": [
    "# BINARIZE train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "cdD93u9zf85z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!tail -n +4 sp/combined_model_2000.vocab | cut -f1 | sed 's/$/ 100/g' > sp/fs.combined_2000.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCwS9xuPf850",
    "outputId": "676beb9e-2e5f-493a-c7e1-aca5b852330b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-29 21:05:33 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "/workspace/fairseq/fairseq/tasks/multires_hubert_pretraining.py:154: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  dictionaries = [ (Dictionary.load(f\"{label_dir}/dict.{label}.txt\") if label is not \"\" else None ) for label in self.cfg.labels]\n",
      "2024-08-29 21:05:37 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='dyu', target_lang='fr', trainpref='pretraining/sp/train', validpref='pretraining/sp/dev', testpref=None, align_suffix=None, destdir='pretraining/bin', thresholdtgt=0, thresholdsrc=0, tgtdict='sp/fs.combined_2000.vocab', srcdict='sp/fs.combined_2000.vocab', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
      "2024-08-29 21:05:37 | INFO | fairseq_cli.preprocess | [dyu] Dictionary: 2001 types\n",
      "2024-08-29 21:05:39 | INFO | fairseq_cli.preprocess | [dyu] pretraining/sp/train.dyu: 18866 sents, 288144 tokens, 0.698% replaced (by <unk>)\n",
      "2024-08-29 21:05:39 | INFO | fairseq_cli.preprocess | [dyu] Dictionary: 2001 types\n",
      "2024-08-29 21:05:39 | INFO | fairseq_cli.preprocess | [dyu] pretraining/sp/dev.dyu: 10 sents, 178 tokens, 0.0% replaced (by <unk>)\n",
      "2024-08-29 21:05:39 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 2001 types\n",
      "2024-08-29 21:05:41 | INFO | fairseq_cli.preprocess | [fr] pretraining/sp/train.fr: 18866 sents, 309173 tokens, 2.0% replaced (by <unk>)\n",
      "2024-08-29 21:05:41 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 2001 types\n",
      "2024-08-29 21:05:41 | INFO | fairseq_cli.preprocess | [fr] pretraining/sp/dev.fr: 10 sents, 154 tokens, 0.0% replaced (by <unk>)\n",
      "2024-08-29 21:05:41 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to pretraining/bin\n"
     ]
    }
   ],
   "source": [
    "#Ensure that you binarize the train_df using the correct paths:\n",
    "!mkdir -p pretraining/bin\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang dyu --target-lang fr \\\n",
    "    --trainpref pretraining/sp/train \\\n",
    "    --validpref pretraining/sp/dev \\\n",
    "    --tgtdict \"sp/fs.combined_2000.vocab\"  \\\n",
    "    --srcdict \"sp/fs.combined_2000.vocab\"  \\\n",
    "    --destdir pretraining/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Q75ogt4f850"
   },
   "source": [
    "# TRAIN MODEL\n",
    "\n",
    "      #--weight-decay 1e-4 \\\n",
    "      #--warmup-updates 2000 \\\n",
    "      #--share-decoder-input-output-embed \\\n",
    "      #--share-all-embeddings \\\n",
    "      \n",
    "      # Set environment variables to limit threading\n",
    "      #!export OMP_NUM_THREADS=1\n",
    "      #!export CUDA_VISIBLE_DEVICES=0\n",
    "      #!export NUMEXPR_MAX_THREADS=1\n",
    "      # --patience 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1YgYx_1f851",
    "outputId": "d4aa4f78-e076-48de-8caf-739b6dcab816",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-29 21:05:45 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-08-29 21:05:49 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='fixed', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=32, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=50, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=1, keep_best_checkpoints=1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='pretraining/bin', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, adam_betas=(0.9, 0.999), adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, encoder_embed_dim=256, encoder_ffn_embed_dim=512, encoder_layers=2, encoder_attention_heads=8, encoder_learned_pos=True, decoder_embed_dim=256, decoder_ffn_embed_dim=512, decoder_layers=2, decoder_attention_heads=8, decoder_learned_pos=True, no_seed_provided=False, encoder_embed_path=None, encoder_normalize_before=False, decoder_embed_path=None, decoder_normalize_before=False, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'pretraining/bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2024-08-29 21:05:49 | INFO | fairseq.tasks.translation | [dyu] dictionary: 2001 types\n",
      "2024-08-29 21:05:49 | INFO | fairseq.tasks.translation | [fr] dictionary: 2001 types\n",
      "2024-08-29 21:05:49 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(2001, 256, padding_idx=1)\n",
      "    (embed_positions): LearnedPositionalEmbedding(1026, 256, padding_idx=1)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(2001, 256, padding_idx=1)\n",
      "    (embed_positions): LearnedPositionalEmbedding(1026, 256, padding_idx=1)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=256, out_features=2001, bias=False)\n",
      "  )\n",
      ")\n",
      "2024-08-29 21:05:49 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2024-08-29 21:05:49 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2024-08-29 21:05:49 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
      "2024-08-29 21:05:49 | INFO | fairseq_cli.train | num. shared model params: 4,697,856 (num. trained: 4,697,856)\n",
      "2024-08-29 21:05:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2024-08-29 21:05:49 | INFO | fairseq.data.data_utils | loaded 10 examples from: pretraining/bin/valid.dyu-fr.dyu\n",
      "2024-08-29 21:05:49 | INFO | fairseq.data.data_utils | loaded 10 examples from: pretraining/bin/valid.dyu-fr.fr\n",
      "2024-08-29 21:05:49 | INFO | fairseq.tasks.translation | pretraining/bin valid dyu-fr 10 examples\n",
      "2024-08-29 21:05:50 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2024-08-29 21:05:50 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 44.352 GB ; name = NVIDIA A40                              \n",
      "2024-08-29 21:05:50 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2024-08-29 21:05:50 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2024-08-29 21:05:50 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 32\n",
      "2024-08-29 21:05:50 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
      "2024-08-29 21:05:50 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
      "2024-08-29 21:05:50 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2024-08-29 21:05:50 | INFO | fairseq.data.data_utils | loaded 18,866 examples from: pretraining/bin/train.dyu-fr.dyu\n",
      "2024-08-29 21:05:50 | INFO | fairseq.data.data_utils | loaded 18,866 examples from: pretraining/bin/train.dyu-fr.fr\n",
      "2024-08-29 21:05:50 | INFO | fairseq.tasks.translation | pretraining/bin train dyu-fr 18866 examples\n",
      "2024-08-29 21:05:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-08-29 21:05:50 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2024-08-29 21:05:50 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2024-08-29 21:05:50 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2024-08-29 21:05:50 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
      "2024-08-29 21:05:50 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2024-08-29 21:05:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-08-29 21:05:50 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2024-08-29 21:05:50 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2024-08-29 21:05:50 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2024-08-29 21:05:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 590\n",
      "epoch 001:   0%|                                        | 0/590 [00:00<?, ?it/s]2024-08-29 21:05:50 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2024-08-29 21:05:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/workspace/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 001:   0%|                                | 1/590 [00:00<04:04,  2.41it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "epoch 001: 100%|▉| 588/590 [00:21<00:00, 31.70it/s, loss=4.907, ppl=29.99, wps=12024-08-29 21:06:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-08-29 21:06:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-08-29 21:06:12 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.397 | ppl 21.07 | wps 0 | wpb 101 | bsz 8 | num_updates 590\n",
      "2024-08-29 21:06:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 590 updates\n",
      "2024-08-29 21:06:12 | INFO | fairseq.trainer | Saving checkpoint to /workspace/checkpoints/checkpoint_best.pt\n",
      "2024-08-29 21:06:12 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/checkpoints/checkpoint_best.pt\n",
      "2024-08-29 21:06:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 590 updates, score 4.397) (writing took 0.5897028632462025 seconds)\n",
      "2024-08-29 21:06:12 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2024-08-29 21:06:12 | INFO | train | epoch 001 | loss 5.147 | ppl 35.44 | wps 14311.6 | ups 27.32 | wpb 523.9 | bsz 32 | num_updates 590 | lr 0.0005 | gnorm 1.707 | train_wall 20 | gb_free 44.2 | wall 23\n",
      "2024-08-29 21:06:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-08-29 21:06:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 590\n",
      "epoch 002:   0%|                                        | 0/590 [00:00<?, ?it/s]2024-08-29 21:06:12 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2024-08-29 21:06:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  61%|▌| 357/590 [00:12<00:08, 27.30it/s, loss=4.196, ppl=18.32, wps=1"
     ]
    }
   ],
   "source": [
    "!fairseq-train pretraining/bin \\\n",
    "  --arch transformer \\\n",
    "  --task translation \\\n",
    "  --dropout 0.1 \\\n",
    "  --attention-dropout 0.1 \\\n",
    "  --activation-dropout 0.1 \\\n",
    "  --encoder-embed-dim 256 \\\n",
    "  --encoder-ffn-embed-dim 512 \\\n",
    "  --encoder-layers 2 \\\n",
    "  --encoder-attention-heads 8 \\\n",
    "  --encoder-learned-pos \\\n",
    "  --decoder-embed-dim 256 \\\n",
    "  --decoder-ffn-embed-dim 512 \\\n",
    "  --decoder-layers 2 \\\n",
    "  --decoder-attention-heads 8 \\\n",
    "  --decoder-learned-pos \\\n",
    "  --max-epoch 50 \\\n",
    "  --optimizer adam \\\n",
    "  --lr 5e-4 \\\n",
    "  --batch-size 32 \\\n",
    "  --seed 1 \\\n",
    "  --save-dir checkpoints \\\n",
    "  --no-epoch-checkpoints \\\n",
    "  --keep-last-epochs 1 \\\n",
    "  --keep-best-checkpoints 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cs9jK7Cf858",
    "outputId": "2f56a4c3-2de8-46f0-bedb-84eb5287130c"
   },
   "outputs": [],
   "source": [
    "!sync  # This forces the system to flush the file system buffers\n",
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulZMtY2af859"
   },
   "source": [
    "# EVALUATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0rKyMbzf859",
    "outputId": "7e395b94-7027-4151-cfac-c5a3e6796717"
   },
   "outputs": [],
   "source": [
    "# ckpt_best, beam=5\n",
    "!fairseq-generate pretraining/bin \\\n",
    "    --path checkpoints/checkpoint_best.pt \\\n",
    "    --batch-size 32 \\\n",
    "    --beam 5 \\\n",
    "    --seed 1 \\\n",
    "    --scoring sacrebleu  \\\n",
    "    --source-lang dyu \\\n",
    "    --target-lang fr \\\n",
    "    --gen-subset valid \\\n",
    "    #--wandb-project \"Dyula to French Translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2OVVmIvYf859",
    "outputId": "f6d83648-2536-46fb-ea30-d66a82506f76"
   },
   "outputs": [],
   "source": [
    "# ckpt_best, beam=10\n",
    "!fairseq-generate pretraining/bin \\\n",
    "    --path checkpoints/checkpoint_best.pt \\\n",
    "    --batch-size 32 \\\n",
    "    --beam 10 \\\n",
    "    --seed 1 \\\n",
    "    --scoring sacrebleu  \\\n",
    "    --source-lang dyu \\\n",
    "    --target-lang fr \\\n",
    "    --gen-subset valid \\\n",
    "    #--wandb-project \"Dyula to French Translation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIt8meXUGiYf"
   },
   "source": [
    "# ENSEMBLE CHECKPOINTS (AVEREAGE CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKPNGRfjf85-",
    "outputId": "e02746c0-9544-4b8a-f897-3a04d041adf8"
   },
   "outputs": [],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_szOyTrh7dT",
    "outputId": "81ac3b57-2d59-4c1e-ef2a-6933d0803516"
   },
   "outputs": [],
   "source": [
    "#! python fairseq/scripts/average_checkpoints.py --inputs checkpoints/ --num-epoch-checkpoints 5 --output checkpoints/averaged.pt\n",
    "!python fairseq/scripts/average_checkpoints.py --inputs checkpoints/checkpoint_best.pt checkpoints/checkpoint_last.pt --output checkpoints/averaged.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_UsfJsff85-",
    "outputId": "45a22274-2681-4773-b48a-a01bbf725392"
   },
   "outputs": [],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSrtt_IFf85-",
    "outputId": "9a4d41fe-1fc9-4349-92b2-ce43237c80c0"
   },
   "outputs": [],
   "source": [
    "# ckpt_avg, beam=5\n",
    "!fairseq-generate pretraining/bin \\\n",
    "    --path checkpoints/averaged.pt \\\n",
    "    --batch-size 32 \\\n",
    "    --beam 10 \\\n",
    "    --seed 1 \\\n",
    "    --scoring sacrebleu  \\\n",
    "    --source-lang dyu \\\n",
    "    --target-lang fr \\\n",
    "    --gen-subset valid \\\n",
    "    #--wandb-project \"Dyula to French Translation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F12YaRWtHSkQ"
   },
   "source": [
    "# EXPORT THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opOQi-zmip5y"
   },
   "outputs": [],
   "source": [
    "!cp pretraining/bin/dict.dyu.txt $trained_model_dir/dict.dyu.txt  # Copy the Dyula dictionary\n",
    "!cp pretraining/bin/dict.fr.txt $trained_model_dir/dict.fr.txt    # Copy the French dictionary\n",
    "!cp checkpoints/averaged.pt $trained_model_dir/model.pt  # Copy the best model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkJYbkdFf85_",
    "outputId": "7e141d75-4b27-4c55-8a39-fafe4a08dda2"
   },
   "outputs": [],
   "source": [
    "ls -l trained_model/model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "bm6ehyrGniP6",
    "outputId": "217ffa95-5d3b-4542-ed8c-eeb7ca1f316b"
   },
   "outputs": [],
   "source": [
    "\"\"\"! fairseq-interactive \\\n",
    "  --path trained_model/model.pt \\\n",
    "  --source-lang dyu --target-lang fr \\\n",
    "  --tokenizer moses \\\n",
    "  --task translation -- gpu 0 \\\n",
    "  --beam 10 \\\n",
    "  trained_model/\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izsUnpcwf86A"
   },
   "source": [
    "# EXPORT MODEL TO CTRANSLATE COMPATIBLE\n",
    "https://opennmt.net/CTranslate2/guides/fairseq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tD4IGOxfFj0",
    "outputId": "137db64a-281d-475a-b36e-92da31d0d462"
   },
   "outputs": [],
   "source": [
    "!ct2-fairseq-converter \\\n",
    "  --model_path trained_model/model.pt \\\n",
    "  --train_df_dir pretraining/bin \\\n",
    "  --force \\\n",
    "  --output_dir destination_dir \n",
    "  --quantization int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l ct2_model/model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "Q-fkjY2Xl_lM",
    "outputId": "4dca5329-9e7d-429f-84a0-8229154dbf97"
   },
   "outputs": [],
   "source": [
    "# Load the CTranslate2 model\n",
    "translator = ctranslate2.Translator(\"destination_dir\")\n",
    "\n",
    "# Load SentencePiece model if used for tokenization\n",
    "sp = spm.SentencePieceProcessor(model_file=\"sp/combined_model_2000.model\")\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Ni kêla tagafènyé inika ordinatairi walama tableti, fou ika copi dôw mara a hakili walama diski kônôn (mi lasôrô kadi ali ni interneti ti bolo).\"\n",
    "reference_sentence = \"Si vous voyagez avec un ordinateur portable ou une tablette, conservez une copie dans sa mémoire ou son disque (accessibles hors connexion).\"\n",
    "# Tokenize the sentence using SentencePiece\n",
    "tokens = sp.encode(sentence, out_type=str)\n",
    "print(\"The Tokens: \",tokens)\n",
    "\n",
    "# Perform translation\n",
    "translation = translator.translate_batch([tokens], beam_size=5)\n",
    "\n",
    "# Decode tokens back to text\n",
    "translated_sentence = sp.decode(translation[0].hypotheses[0])\n",
    "print(\"Input Sentence:\",sentence,\"\\n\")\n",
    "print(\"The Translation: \",translated_sentence,\"\\n\")\n",
    "print(\"The Reference: \",reference_sentence,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaRIDEKmf86B"
   },
   "outputs": [],
   "source": [
    "# Specify model directory and load models\n",
    "MODEL_DIR = r\"destination_dir\"\n",
    "translator = ctranslate2.Translator(MODEL_DIR)\n",
    "sp = spm.SentencePieceProcessor(model_file=\"sp/combined_model_2000.model\")\n",
    "\n",
    "# Regular expression to clean text\n",
    "CHARS_TO_REMOVE_REGEX = r'[!\"&\\(\\),-./:;=?+.\\n\\[\\]«»]'\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean input text by removing special characters and converting to lowercase.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(CHARS_TO_REMOVE_REGEX, ' ', text)\n",
    "    text = re.sub(r'\\.{2,}', '', text)\n",
    "    text = text.replace(\"’\", \"'\")\n",
    "    text = text.replace('—', '')\n",
    "    text = text.replace('…', '')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Prepare your input sentences and reference translations\n",
    "input_sentences = validation_df.dyu.tolist()\n",
    "reference_translations = validation_df.fr.tolist()\n",
    "\n",
    "# Lists to accumulate translations and references for cumulative sacrebleu \n",
    "translated_sentences = []\n",
    "cleaned_references = []\n",
    "\n",
    "def translate_and_evaluate(input_sentence, reference_translation):\n",
    "    \"\"\"Translate the input sentence and evaluate against the reference.\"\"\"\n",
    "    prepared_train_df = clean_text(input_sentence)\n",
    "    tokens = sp.encode(prepared_train_df, out_type=str)\n",
    "    translation = translator.translate_batch([tokens], beam_size=5)\n",
    "    translated_sentence = sp.decode(translation[0].hypotheses[0])\n",
    "\n",
    "    # Append translated and cleaned reference to lists for cumulative sacrebleu \n",
    "    translated_sentences.append(translated_sentence)\n",
    "    cleaned_reference = clean_text(reference_translation)\n",
    "    cleaned_references.append(cleaned_reference)\n",
    "\n",
    "    # Calculate sentence-level sacrebleu  score using sacrebleu\n",
    "    sentence_sacrebleu = sacrebleu.sentence_bleu(translated_sentence, [cleaned_reference])\n",
    "\n",
    "    # Print individual sentence results\n",
    "    print(f\"Input: {input_sentence}\")\n",
    "    print(f\"Translated: {translated_sentence}\")\n",
    "    print(f\"Reference: {reference_translation}\")\n",
    "    print(f\"Sentence SacreBLEU Score: {sentence_sacrebleu.score}\\n\")\n",
    "    print(\"-\" * 50, \"END\", \"-\" * 50)\n",
    "\n",
    "# Measure time for translations\n",
    "start = time.time()\n",
    "\n",
    "# Process each sentence for translation and individual sacrebleu evaluation\n",
    "for input_sentence, reference_translation in zip(input_sentences, reference_translations):\n",
    "    translate_and_evaluate(input_sentence, reference_translation)\n",
    "\n",
    "end = time.time()\n",
    "spent = end - start\n",
    "print(\"TIME SPENT:\", spent)\n",
    "\n",
    "# Calculate and print cumulative sacrebleu score\n",
    "cumulative_sacrebleu = sacrebleu.corpus_bleu(translated_sentences, [cleaned_references])\n",
    "print(f\"Cumulative SacreBLEU Score: {cumulative_sacrebleu.score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuHZl7hff86C"
   },
   "source": [
    "# PUSH TO HUGGING FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyDoIbx2f86C"
   },
   "outputs": [],
   "source": [
    "# Optionally add a model card\n",
    "# Create the config\n",
    "model_card = f\"\"\"---\n",
    "---\n",
    "language:\n",
    "- dyu\n",
    "- fr\n",
    "- multilingual\n",
    "tags:\n",
    "- translation\n",
    "- ctranslate2\n",
    "model-index:\n",
    "- name: JJuliusFx/dyula-french-ctranslate2_v2_50epochs\n",
    "  results: []\n",
    "---\n",
    "\n",
    "# JuliusFx/dyu-fr-ctranslate2\n",
    "\n",
    "This is a machine translation model that translates Dyula to French using the [ctranslate2 framework](https://github.com/ctranslate2/ctranslate2).\n",
    "\n",
    "The modelbased on [this google colab notebook](https://colab.research.google.com/drive/1HcR35DLkNszQyGv_29bYVR9EMVwERqGI?usp=sharing#scrollTo=fbagjxYmZ9rC) that was kindly created by [Kartheek Akella](https://www.youtube.com/redirect?event=channel_description&redir_token=QUFFLUhqbm5UWVY3RG1ObmVOWUxVV2lZdUtPLWtwVmxRZ3xBQ3Jtc0tsNV9wd3Uyem5hQUFoQ0x5eTBxN29QYUwweWV0T3QxTzk2WmpxQktHMk1VZE1pMlRteFVaUDdxMTdmQ3ZfNTNXSVlRMDRrd1V1a1RQNmJrcGVhSkpsTzBNTi1MOUtQTmpMdDhUcjdpTnNhT1VKWGs2VQ&q=https%3A%2F%2Fasvskartheek.github.io%2F).\n",
    "\n",
    "## Model description\n",
    "\n",
    "More information needed\n",
    "\n",
    "## Intended uses & limitations\n",
    "\n",
    "More information needed\n",
    "\n",
    "## Training and evaluation train_df\n",
    "\n",
    "More information needed\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Load and use for inference\n",
    "\n",
    "```python\n",
    "import ctranslate2\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Download model\n",
    "snapshot_download(\n",
    "    repo_id=\"JuliusFx/dyula-french-ctranslate2_v2_25epochs\",\n",
    "    local_dir=\"/path/to/save/locally\"\n",
    ")\n",
    "\n",
    "# Define model interface\n",
    "class CTranslate2Model:\n",
    "    def __init__(self, model_dir: str, sp_model_path: str) -> None:\n",
    "        self.translator = ctranslate2.Translator(model_dir)\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(sp_model_path)\n",
    "\n",
    "    def translate(self, sentence: str) -> str:\n",
    "        '''\n",
    "        Translate the given sentence.\n",
    "\n",
    "        :param sentence: Sentence to be translated\n",
    "        :return: Translated sentence.\n",
    "        '''\n",
    "        prepared_train_df = clean_text(sentence)\n",
    "        tokens = self.sp.Encode(prepared_train_df, out_type=str)\n",
    "        translation = self.translator.translate_batch([tokens], beam_size=5)\n",
    "        translated_sentence = self.sp.Decode(translation[0].hypotheses[0])\n",
    "        return translated_sentence\n",
    "\n",
    "# Initialize the model once\n",
    "model_dir = \"/path/to/your/model_dir\"\n",
    "sp_model_path = \"/path/to/your/sp_model_path\"\n",
    "translator = CTranslate2Model(model_dir=model_dir, sp_model_path=sp_model_path)\n",
    "\n",
    "def translate_sentence(sentence: str) -> str:\n",
    "    return translator.translate(sentence)\n",
    "\n",
    "# Example usage\n",
    "dyula_sentence = \"i tɔgɔ bi cogodɔ\"\n",
    "translated_sentence = translate_sentence(dyula_sentence)\n",
    "print(\"Dyula sentence:\", dyula_sentence)\n",
    "print(\"Translated French sentence:\", translated_sentence)\n",
    "```\n",
    "\n",
    "## Training procedure\n",
    "\n",
    "### Training hyperparameters\n",
    "\n",
    "More information needed\n",
    "\n",
    "### Training results\n",
    "\n",
    "More information needed\n",
    "\n",
    "### Framework versions\n",
    "\n",
    "\"\"\"\n",
    "with (Path(\"destination_dir\") / \"README.md\").open('w') as f:\n",
    "    f.write(model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk8ocWdaf86C"
   },
   "outputs": [],
   "source": [
    "# List files in the model directory (lean_model)\n",
    "files = []\n",
    "for filename in os.listdir(destination_dir):\n",
    "    filepath = os.path.join(destination_dir, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        files.append(Path(filepath))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzRao-Q2f86D"
   },
   "outputs": [],
   "source": [
    "# Define source paths for each file\n",
    "files_to_copy = [\n",
    "    # Uncomment and add more files as needed\n",
    "    #Path('../ct2_model/README.md'),\n",
    "    Path('sp/combined_model_2000.model')\n",
    "]\n",
    "\n",
    "# Define destination directory as a Path object\n",
    "destination_dir = Path(\"destination_dir\")\n",
    "\n",
    "# Create the destination directory if it does not exist\n",
    "destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy files to the destination directory\n",
    "for file in files_to_copy:\n",
    "    if file.exists():\n",
    "        shutil.copy(file, destination_dir)\n",
    "\n",
    "# List files in the destination directory to confirm the copy\n",
    "copied_files = list(destination_dir.glob('*'))\n",
    "print(\"Files in destination directory:\")\n",
    "for f in copied_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mjy0JM3f86D"
   },
   "outputs": [],
   "source": [
    "for file_path in copied_files:\n",
    "    print(file_path.name)\n",
    "    print(str(file_path))\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=file_path,\n",
    "        path_in_repo=file_path.name,\n",
    "        repo_id=hf_repo,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1df6de2e52fd4666b14604ed07ff4a34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2723d4d96f85429d966c2cc727840318": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7712ab57846b4b0f8492ff04d90cc5d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78e3559e9ad1451f90a8a08e5c43fa9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2c0a2f341624db4943adae852fb266f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1df6de2e52fd4666b14604ed07ff4a34",
      "max": 2009,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2723d4d96f85429d966c2cc727840318",
      "value": 2009
     }
    },
    "ad7cb16de5b44e24bfe80aeaa06d56bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c72b8dedae1741b1b9d614e77c6d4c34",
      "placeholder": "​",
      "style": "IPY_MODEL_bdd8f2e5580242d0943a855a96d9816a",
      "value": " 2009/2009 [00:00&lt;00:00, 13759.60 examples/s]"
     }
    },
    "bdd8f2e5580242d0943a855a96d9816a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c72b8dedae1741b1b9d614e77c6d4c34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da1224d9e51d4fccbaf4e531d682acca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3fa9f8f02bd4be38f90e330629b5d40",
      "placeholder": "​",
      "style": "IPY_MODEL_7712ab57846b4b0f8492ff04d90cc5d8",
      "value": "Map: 100%"
     }
    },
    "f3fa9f8f02bd4be38f90e330629b5d40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fea0c640727140b59bd8d16feb2ea797": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da1224d9e51d4fccbaf4e531d682acca",
       "IPY_MODEL_a2c0a2f341624db4943adae852fb266f",
       "IPY_MODEL_ad7cb16de5b44e24bfe80aeaa06d56bb"
      ],
      "layout": "IPY_MODEL_78e3559e9ad1451f90a8a08e5c43fa9d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
